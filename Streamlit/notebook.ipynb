{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import  stopwords\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download necessary Natural language toolkit libriries for lemmen and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(['punkt','punkt_tab','stopwords','wordnet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "separate data into train test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('https://raw.githubusercontent.com/Jana-Liebenberg/2401PTDS_Classification_Project/main/Data/processed/train.csv', sep=',', encoding='utf-8')\n",
    "test_df = pd.read_csv('https://raw.githubusercontent.com/Jana-Liebenberg/2401PTDS_Classification_Project/main/Data/processed/test.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View dimensions of graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identify the ratio split of news category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['category'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Format data\n",
    "in the following order:\n",
    "1) remove punctuation and standardise text format\n",
    "2) tokenise text\n",
    "3) remove stop words\n",
    "4) lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to remove punctuation marks:\n",
    "def remove_punctuation_and_numbers(text):\n",
    "    \n",
    "    #convert corpus into lowercase text \n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove punction marks \n",
    "    text = ''.join([word for word in text if word not in string.punctuation and not word.isdigit()])\n",
    "\n",
    "    #regex to remove punctions not included in string library\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)     # used to remove numbers and other special characters omitted  \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to tokenize text in dataframe\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    word_tokens = word_tokenize(text)\n",
    "    return word_tokens \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to remove stop words\n",
    "def remove_stop_words(tokens):\n",
    "    \n",
    "    filtered_tokenz = [word for word in tokens if word not in stopwords_list and len(word) >= 3]\n",
    "    return filtered_tokenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to lemmentize text\n",
    "def lemmentize_words(tokens):\n",
    "\n",
    "    lemmentizer = WordNetLemmatizer()\n",
    "    lemmentized_words = [lemmentizer.lemmatize(word ,pos='n') for word in tokens]\n",
    "\n",
    "    return lemmentized_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(df):\n",
    "\n",
    "    df = remove_punctuation_and_numbers(df)\n",
    "    \n",
    "    df = tokenize_text(df)\n",
    "    \n",
    "    df = remove_stop_words(df)\n",
    "    \n",
    "    df = lemmentize_words(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['headlines'] = train_df['headlines'].apply(format_data)\n",
    "train_df['description'] = train_df['description'].apply(format_data)\n",
    "train_df['content'] = train_df['content'].apply(format_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Check the quality and consistancy of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Feature Engeneering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify relevant features \n",
    "print(train_df.iloc[0]['category'])\n",
    "print(train_df.iloc[0]['headlines'])\n",
    "print(train_df.iloc[0]['description'])\n",
    "print(train_df.iloc[0]['content'])\n",
    "\n",
    "print(train_df.iloc[2]['category'])\n",
    "print(train_df.iloc[2]['headlines'])\n",
    "print(train_df.iloc[2]['description'])\n",
    "print(train_df.iloc[2]['content'])\n",
    "\n",
    "print(train_df.iloc[4]['category'])\n",
    "print(train_df.iloc[4]['headlines'])\n",
    "print(train_df.iloc[4]['description'])\n",
    "print(train_df.iloc[4]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement one-hot encoding for target variables:\n",
    "\n",
    "y = pd.get_dummies(train_df['category'],dtype=int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_category_classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
