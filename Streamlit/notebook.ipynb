{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnEOl58CIvYA"
      },
      "source": [
        "Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "0TIunihFIvYB"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_DUNRdNIvYC"
      },
      "source": [
        "download necessary Natural language toolkit libriries for lemmen and tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PooU5-SIvYC",
        "outputId": "95f74a19-815b-4008-907f-8e679cccc2b6"
      },
      "outputs": [],
      "source": [
        "nltk.download(['punkt','punkt_tab','stopwords'], quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX7xLWl8IvYC"
      },
      "source": [
        "separate data into train test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "U5yX850VIvYC"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('https://raw.githubusercontent.com/Jana-Liebenberg/2401PTDS_Classification_Project/main/Data/processed/train.csv', sep=',', encoding='utf-8')\n",
        "test_df = pd.read_csv('https://raw.githubusercontent.com/Jana-Liebenberg/2401PTDS_Classification_Project/main/Data/processed/test.csv', sep=',', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "BUw6dDOEIvYD",
        "outputId": "2cbbe532-374b-446d-fdec-82c4d8ddc3d8"
      },
      "source": [
        "View dimensions of graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "BUw6dDOEIvYD",
        "outputId": "2cbbe532-374b-446d-fdec-82c4d8ddc3d8"
      },
      "outputs": [],
      "source": [
        "train_plot = train_df['category'].value_counts().plot(kind='bar')\n",
        "test_plot = test_df['category'].value_counts().plot(kind='bar')\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYcNUhQlIvYD"
      },
      "source": [
        "### 1) Format data\n",
        "in the following order:\n",
        "1) test on headline data given it is easier to split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "QyKYMAVtIvYD"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_punctuation_numbers_and_stopwords(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    pun_nums = string.punctuation + \"0123456789\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    set_to_remove = stop_words.union(set(pun_nums))\n",
        "    word_tokens = word_tokenize(text)   #Tokenize text to find stopwords more easily \n",
        "\n",
        "    filterd_text = \" \".join([w for w in word_tokens if w not in set_to_remove])\n",
        "\n",
        "    second_check = re.sub(r'\\d','',filterd_text)\n",
        "\n",
        "    return second_check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "QyKYMAVtIvYD"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Here we remove any punctuations and numbers that might have been in the data using the method declared\n",
        "before for:\n",
        "\"\"\"\n",
        "\n",
        "#Headlines Data:\n",
        "train_df['headlines'] = train_df['headlines'].apply(remove_punctuation_numbers_and_stopwords)\n",
        "test_df['headlines'] = test_df['headlines'].apply(remove_punctuation_numbers_and_stopwords)\n",
        "\n",
        "#Description Data:\n",
        "train_df['description'] = train_df['description'].apply(remove_punctuation_numbers_and_stopwords)\n",
        "test_df['description'] = test_df['description'].apply(remove_punctuation_numbers_and_stopwords)\n",
        "\n",
        "#Content Data:\n",
        "train_df['content'] = train_df['content'].apply(remove_punctuation_numbers_and_stopwords)\n",
        "test_df['content'] = test_df['content'].apply(remove_punctuation_numbers_and_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXDla2UOIvYD",
        "outputId": "66890212-1809-40de-aba5-ae1395417324"
      },
      "source": [
        "### 2) Feature Engeneering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqLhsil2IvYD",
        "outputId": "61a267cc-e286-43a8-bb7d-f426b1c1d7a7"
      },
      "outputs": [],
      "source": [
        "\"\"\"\"\n",
        "Here we format our target variables:\n",
        "\n",
        "\"\"\"\n",
        "y_train = train_df['category']\n",
        "y_test = test_df['category']\n",
        "\n",
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqLhsil2IvYD",
        "outputId": "61a267cc-e286-43a8-bb7d-f426b1c1d7a7"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Here we format our featurs:\n",
        "\n",
        "we implement the TfidfVectorizer\n",
        "\n",
        "\"\"\"\n",
        "def vectorize_data(column):\n",
        "    vect = CountVectorizer(stop_words='english',min_df=0.01)\n",
        "    X_train = vect.fit_transform(train_df[column]).toarray()       #we use .toarray() to convert the matrix into a numpy array\n",
        "    X_test = vect.transform(test_df[column]).toarray()\n",
        "    vocabulary = vect.get_feature_names_out()\n",
        "\n",
        "    #Pickel Vectoriser:\n",
        "    with open('.vectoriser_file/vect.pkl', 'wb') as file:\n",
        "        pickle.dump(vect, file)\n",
        "\n",
        "    return X_train, X_test, vocabulary\n",
        "\n",
        "X_headlines_train, X_headlines_test, headlines_vocabulary = vectorize_data('headlines')\n",
        "X_description_train, X_description_test, description_vocabulary = vectorize_data('description')\n",
        "X_content_train, X_content_test, content_vocabulary = vectorize_data('content')\n",
        "\n",
        "\n",
        "print('X_headlines-train:',X_headlines_train.shape)\n",
        "print('X_headlines-test:',X_headlines_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gztVeYJQIoZ"
      },
      "source": [
        "### 3) Model training  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def fit_random_forest(X_train, y_train, X_test, y_test):\n",
        "   \n",
        "    rf_model = RandomForestClassifier()\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    rf_cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "\n",
        "    #Generate classification report\n",
        "    classification_report_str = classification_report(y_test, y_pred)\n",
        "\n",
        "    return rf_model, rf_cm, accuracy, classification_report_str\n",
        "\n",
        "\n",
        "\n",
        "rf_model, rf_cm, rf_accuracy, rf_report = fit_random_forest(X_content_train, y_train, X_content_test, y_test)\n",
        "\n",
        "print('Random Forest Accuracy:', rf_accuracy)\n",
        "print(\"\\nClassification Report:\\n\", rf_report)\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Pickel the model:\n",
        "with open('.models_file/rf_model.pkl', 'wb') as file:\n",
        "    pickle.dump(rf_model, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_multinomial_nb_multiclass(X_train, y_train, X_test, y_test):\n",
        "    \n",
        "    nb_model = MultinomialNB()  # Initialize the Multinomial Naive Bayes model\n",
        "    nb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = nb_model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    nb_cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Generate classification report\n",
        "    classification_report_str = classification_report(y_test, y_pred)\n",
        "\n",
        "    return nb_model, nb_cm, accuracy, classification_report_str\n",
        "\n",
        "\n",
        "\n",
        "nb_model, nb_cm, nb_accuracy, nb_report = fit_multinomial_nb_multiclass(X_content_train, y_train, X_content_test, y_test)\n",
        "\n",
        "print('Multinomial Naive Bayes Multi-Class Accuracy:', nb_accuracy)\n",
        "print(\"\\nClassification Report:\\n\", nb_report)\n",
        "\n",
        "# Pickel the model:\n",
        "\n",
        "with open('.models_file/nb_model.pkl', 'wb') as file:\n",
        "    pickle.dump(nb_model, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lets get a visual breakdown:\n",
        " \n",
        "def display_matrix_graph(class_names=None, rf_cm=None, nb_cm=None):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))  # Create subplots\n",
        "\n",
        "    sns.heatmap(rf_cm, annot=True, fmt=\"d\", cmap=\"Reds\", \n",
        "                xticklabels=class_names, yticklabels=class_names, ax=axes[0])\n",
        "    axes[0].set_xlabel(\"Predicted\")\n",
        "    axes[0].set_ylabel(\"True\")\n",
        "    axes[0].set_title(\"Random Forest Confusion Matrix\")\n",
        "\n",
        "    sns.heatmap(nb_cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
        "                xticklabels=class_names, yticklabels=class_names, ax=axes[1])\n",
        "    axes[1].set_xlabel(\"Predicted\")\n",
        "    axes[1].set_ylabel(\"True\")\n",
        "    axes[1].set_title(\"Naive Bayers Confusion Matrix\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "class_names = y_test.unique()\n",
        "display_matrix_graph(class_names,rf_cm,nb_cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lets see the most frequent words in our corpus:\n",
        "\n",
        "vocabulary_map = {\n",
        "    'headlines': headlines_vocabulary,\n",
        "    'description': description_vocabulary,\n",
        "    'content': content_vocabulary,\n",
        "}\n",
        "\n",
        "vocabulary_map"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
